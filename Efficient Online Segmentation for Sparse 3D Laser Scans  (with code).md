Recently, I found that many CSDN blogs are reprinting my articles to attract attention. Here, I have to move my original articles from Bogayuan to CSDN's columns again. At the same time, I hope that the students will respect the results of their labor and indicate the source of the reprint!!! 

 In the application of laser-based autonomous driving or mobile robots, the ability to extract individual objects from a moving scene is very important. Because such systems need to perceive changing or moving objects in a dynamic sensing environment, in the sensing system, preprocessing the image or point cloud data into individual objects is the first step for further analysis. 

 In this paper, a very efficient segmentation method is proposed. First, the scanned point cloud is removed from the plane for processing, and then the point cloud data within a certain range after removing the plane is segmented into different objects. The paper focuses on solving the problem of efficient segmentation on most systems under the condition of small computation. It avoids direct calculation of 3D point clouds and operates directly on 2.5D depth images. This scheme can solve the problem of processing sparse 3D point cloud data well. The author uses a new Velodyne VLP-16 scanner, and the code implements this method in C++ and ROS, and the code is open-source. This method can be run using a core solo CPU and a frame running rate higher than the sensor, and can produce high-quality segmentation results. 

 ![avatar]( 20191219231851395.jpg) 

 Left: Objects (e.g. people, cars, and trees) generated after segmentation using sparse 3D point cloud data obtained by the Velodyne VLP-16 scanner. Different colors correspond to different segmentation results. Right: Clearpath Husky robot used for experiments. 

 Separating individual objects in 3D laser point cloud data is an important task for autonomous navigation by mobile robots or autonomous vehicles. Autonomous vehicles navigating in unknown environments are faced with the complex task of reasoning about their surroundings., On busy streets with cars and pedestrians, maps can be affected by erroneous data associations resulting from the dynamic nature of the environment. A key step in being able to better reason about such objects and ignore possible dynamic objects during scan registration and mapping is to split 3D point cloud data into different objects so that they can be tracked individually 

 Therefore, the important contribution of this paper is to achieve fast reading, efficient and robust segmentation of 3D sparse point clouds. (I personally tested it, it was really fast, my computer's configuration was really bad, but it ran super fast) On a mobile CPU, it can handle Velodyne sensors over 70HZ (64 lines) or 250HZ (16 lines). 

 Ground removal 

 Before segmentation, the ground needs to be removed from the scanned point cloud data. This ground removal method simply removes the 3D points below the height of the vehicle. This method works in simple scenarios, but fails if the pitch or roll angle of the vehicle is not equal to zero or the ground is not a perfect plane. But the situation can be improved using RANSAC's plane fitting method. 

 The lidar provides the distance value of each laser beam, the timestamp, and the direction of the beam of light as the original data source. This allows us to directly convert the data into a depth image. The number of rows in the image is defined by the number of beams of light in the vertical direction. For example, for the Vdyelone scanner, there are 16 lines, 32 lines, and 64 lines, while the number of columns in the image has the distance value obtained by the laser rotation every 360 degrees. Each pixel of this virtual image stores the distance between the sensor and the object. To speed up the calculation, it is even possible to consider combining multiple readings in the horizontal direction into a single pixel if necessary. 

 ![avatar]( 20191219231942296.jpg) 

 Top left: Part of the depth image. Center left: Image generated by displaying the alpha angle. Bottom left: Angle after applying Savitsky-Golay smoothing. Top right: Illustration of the alpha angle. Bottom right: Smooth illustration of the alpha angle column marked in the left image. 

 Using the generated combined image to process instead of directly processing and computing the 3D point cloud can effectively accelerate the processing speed. For other scanners that do not provide distance values, the 3D point cloud can also be projected onto a cylindrical image to calculate the Euclidean distance of each pixel. The method proposed in this paper can still be used. 

 In order to identify the ground, there are three assumptions: 

 1. Assume that the sensor is roughly mounted on a horizontally moving base. 

 2. Assume that the curvature of the ground is low. 

 3. The mobile robot or vehicle observes the ground plane at least at the pixel of the lowest row in the depth image 

 The distance values of each column (c) pixel of the depth image are first converted into angle values ®, which represent the angle of inclination connecting the two points. And respectively represent the depth values adjacent to the row. Knowing the two individual laser beam depth values that are continuously perpendicular, the angle α can be calculated using trigonometry rules as follows: 

 ![avatar]( 20191219232119550.jpg) 

 Where the angles of a and b are the vertical angles of the laser beam corresponding to rows r-1 and r, and since two depth values are required for each α calculation, the resulting angle map size is 1 smaller than the number of rows in the depth map range. It is assumed that all these angles are expressed as the angle values at the r-row and c-column (row and column) coordinates. 

 ![avatar]( 20191219232126184.jpg) 

  Ground recognition algorithm. According to the above algorithm, the ground is marked in light blue. 

 However, since lidar also has error points, it is also necessary to deal with some outliers in the depth range. For details, please refer to the paper. In order to achieve the effect of angular smoothing, the Savitsky-Golay filtering algorithm is used to process each column. After obtaining the filtered angle graph, we start to perform ground recognition on this basis, using breadth-first search to mark similar points together. Breadth-first search (BFS) is a popular graph search traversal algorithm. It starts traversing from a given point in the graph and first starts to explore directly adjacent nodes before moving to the next level. In this paper, we use the N4 domain value on the grid to calculate the angle difference to determine whether two adjacent elements of matrix M satisfy the angular constraint, Delta a, set to 5 °. 

 Fast and efficient segmentation using laser depth images 

 The vertical resolution of the sensor has a very important impact on the difficulty of the segmentation problem. We need to determine whether the laser beam is reflected by the same object for adjacent points. To solve the problem of whether the laser beam is reflected by the same object, here is the method based on angle measurement. The advantage of this method is that the advantages of this method are repeatedly mentioned in the text: First, we can directly use the well-defined neighborhood relationship in the depth image, which makes the segmentation problem easier. Second, we avoid generating 3D point clouds, which makes the overall method faster. 

 The following figure shows the effect of segmentation 

 ![avatar]( 20191219232224978.jpg) 

 This is the result of using this segmentation scheme. (A) The graph is from the Velodynede point cloud. (B) The depth image is created from the original values of the sensor and the ground points have been removed. (C) The graph is the result of the segmentation performed on the basis of the generated depth map. (D) The segmented depth map is restored as a point cloud and displayed in different colors. 

 Here is a detailed explanation of how to implement segmentation using angle constraints: 

 An example scenario as shown in the figure below, in which two people walk close to each other in front of a cyclist, who passes between them and a parked car. The Velodyne VLP-16 scanner used here recorded this scene. The middle image shows the results of two arbitrary points A and B measured from the scanner located at O, indicating the laser beams OA and OB. Without loss of generality, we assume that the coordinates of A and B lie in a coordinate system centered on O, with the y-axis following the longer of the two laser beams. We define the angle β as the angle between the laser beam and the line connecting A and B, which is generally away from the scanner. In practice, angle β proves to be valuable information that can be used to determine whether points A and B are located on the same object. 

 ![avatar]( 20191219232336641.jpg) 

 Then based on the measurement value of the laser, we know the distance value OA of the first measurement and the corresponding second measurement value OB. The measurement results of these two times are marked as d1 and d2 respectively. Then using the above information, the angle can be measured by the following formula: The right illustration in the following figure shows the calculation in the xy plane from the top view of the scene. Note that we can calculate the angle β of the pair of points A and B that are adjacent in the row or column direction in the range image. In the first case, the angle corresponds to the angle increment in the row direction, while in the other case it corresponds to the increment in the column direction. Left: The example scene has two pedestrians, a cyclist and a car. Middle: Assuming the sensor is at point O and the lines OA and OB represent two laser beams, then points A and B produce a line that estimates the surface of the object if they both belong to the same object. We make the judgment of whether it is the same object based on the angle β. If β > theta, where theta is a predetermined threshold, the points are considered to represent an object. Right: Top view of a pedestrian in an example scenario. The green line represents the points where β > theta, while the red line represents the angle below the threshold, so the objects are marked as different. 

 The segmentation method based on the threshold value of β is experimented in experimental evaluation. The actual situation can be the case where the object being scanned is flat, such as a wall, and is almost parallel to the laser beam direction. In this case, the angle β will be small, so the object may be divided into multiple segments. This basically means that if β is less than theta, it is difficult to determine whether the two points originate from two different objects, or are merely located on a flat object that is almost parallel to the beam direction. However, despite this drawback, our experiments show that the method is still useful in practice. The above behavior rarely occurs, and if so, it will usually lead to over-segmentation of particularly inclined planar objects. 

 ![avatar]( 2019121923242596.PNG) 

  The above is about the segmentation algorithm used after going to the ground. It can be seen that the most important formula is the solution of the β angle value 

 experimental part 

 The algorithm (i) performs all calculations quickly, even when running on the core solo of a mobile CPU at approximately 70 Hz. 

 (Ii) can obtain 3D original data sources from mobile robots to generate depth data and segment them into meaningful individuals 

 ![avatar]( 20191219232454275.PNG) 

 (Iii) The method performs well on sparse data, e.g. sparse data obtained from a 16-beam of light Velodyne Puck scanner. In experiments, Euclidean clustering implemented in the Point Cloud Library PCL is used. In all experiments, we use the default parameter theta = 10 °.  

 The code is open source (https://github.com/PRBonn/depth_clustering), and the editor personally tested it, and attached the test video. Those who are interested pay attention to the WeChat official account, and the editor has simplified and optimized the code. Those who are interested can contact and communicate. The original text is on the blog at: https://www.cnblogs.com/li-yao7758258/p/9937704.html 

 And you can recommend valuable code or papers related to point clouds that you think are better, and send them to the group owner's mailbox (dianyunpcl163.com), so that you can communicate and discuss together!! 

 Interested small partners can follow the WeChat official account, join the QQ or WeChat group, and communicate and share with everyone (this group is mainly a communication and sharing group related to point cloud 3D vision, everyone is welcome to join and share) 

 ![avatar]( 20191219232602296.PNG) 

